#
# File:   content-mit-8370x-subtitles/U1L2e.txt
#
# Captions for course module
#
# This file has 379 caption lines.
#
# Do not add or delete any lines.  If there is text missing at the end, please add it to the last line.
#
#----------------------------------------

Now what I want to do is to explain why Maxwell's Daemon
today is realized cannot exist because of a fundamental reason
related to computation.
And this will inform us a great deal in our journey
through quantum computation, because we
want to be aware of these limits of physics
on computation, as we try to realize
a new model of computation.
So the way I will describe this is
with a beautiful construction, called Szilard's engine,
or Szilard, from 1929.
Look at this span of years between these discussions.
There are eight steps in Szilard's engine.
So I will describe each one of them,
but they will be very simple, easy steps.
It may take me a little bit to draw each one of them,
so bear with me.
The basic idea is this configuration,
which will be the core engine.
And this core engine will, again, be a cylinder of gas.
I'm drawing its cross-section, or think
of this as a two-dimensional idealization of it.
There will be a wall that can come down
through a small hole in a partition.
This is supposed to be the same size on both sides.
And imagine that there's just one gas molecule
rumbling around, for sake of simplicity,
inside this idealized vacuum chamber.
In addition to this, I'm going to explicitly add
one thing here, which might seem a little odd.
It's a piece of scratch paper.
It's a little memory.
And we'll see what that piece of memory
is used for in just a moment.
So instead of having a Maxwell daemon sitting
inside a populated gas chamber, we'll start with just this.
And this stage is called the initialization stage.

In the second stage, which we'll call wall lowered,
we have the same configuration.

These are pistons, that later we'll see can slide in and out.
The reason is there are two of them
is because we're going to lower this wall in the middle
at some random time.
And we don't know whether the molecule
is going to be on the right-hand side or the left-hand side
at that time.
So for a sake of concreteness, let's imagine
that at the moment we lower the wall,
the molecule just happens to be on the right-hand side
of the cylinder.
It might have been on the left-hand side.
It's probably equally probable, given a normal distribution
for Maxwell-Boltzmann distribution.
In this third stage, remember our scratchpad
is still in this configuration.
We're going to measure where the molecule is.
So imagine this is just like Maxwell's daemon
looking with his or her eyes at a gas molecule.
Now, we're just going to look at one thing.
Is the molecule on the right-hand side
or on the left-hand side of this double cylinder configuration?
All right, so here we go.
We don't change anything about the cylinder or its two
pistons.
But we do observe this molecule as it's
careening around and bouncing off the walls,
and we see that it's on the right-hand side.

And because it's on the right-hand side,
we're right down an R on our scratchpad memory.
If it had been on the left-hand side,
I would be doing something almost identical
in the next few steps.
The next step is the one which makes a difference
with respect to whether you knew it was on the left
or on the right.
Given that the molecule is the only molecule in the cylinder,
how hard is it for me to move this piston
in on the left-hand side?

Yes, please.
It takes zero effort.
It takes zero effort because nothing is pushing back on it.
There's just a vacuum over here.
There's vacuum everywhere in this world.
So the only thing that's pushing something around
is this one solitary molecule kicking around.
In fact, as it bounces around, it's
probably pushing against this piston too.
But we're holding it still so nothing's happening.
But the key is, as you just said,
it costs us no energy and effort to move this one
piston on the left in, because there's no molecule over there.
And we can do that because we know,
it's on the right-hand side.
So we just compress on this left-hand side.
That's step four.
Step five, now we raise the wall.

So wall goes up.
Now, we have the left piston over here.
And the right piston over here.
And we have this molecule that's still bouncing around,
careening around over here.
And in this step, we have the molecule running around
at some random point on the right-hand side
of this double cylinder.
And because it's bouncing around,
in this next step, which I'll call expand, the wall stays up.
What happens?
Well, this molecule at some point
will randomly have a trajectory, which hits this wall
and bounces off.
And when it hits this wall, this movable wall as a piston,
the piston moves backward like this.
These are very, very light pistons.
And this is a really, really kind of momentous
molecule that's moving around.
And so it goes and goes bing and it moves the piston.
And it goes bing, and the piston moves out further.
You might have to wait a while, but everything in the service
of a PhD thesis, right?

So you can guess what the next step is.
We're at stage six.
We're almost done.
Stage 7, as we let this ping back and forth,
eventually this left piston is going to move all the way back
to its initial state.
So we can say that we are resetting the position.

And this resets the position of the two pistons.
We have the wall out.
The pistons will come back into their initial state over here.
Let me not forget our little scratch pad over here.

And from a mechanical standpoint,
we're nearly back to the initial state.
What do I need to do?
What last thing do I need to do to get back to the very
initial stage of this system?
Somebody else.
I heard it.
Yellow shirt.
Clear the memory.
Clear memory, yeah, what's so hard about that?
Don't you do that all the time?

What's so hard about clearing the memory?

I'm sure you've experienced this before.
You know, it's awfully hard to forget some things.

But this is a single bit.
What could be so hard about forgetting
a single bit of information?
Well, OK, that's awfully hard to forget sometimes too.
Thoughts?
Answers?
Anyone except the two who have answered most of the questions?
How about a student who's wearing a nice Chad Rigetti
shirt.
It takes work.
Why does it take work?

Because you had to put some information.
It's nonrandom.
It's nonrandom.
And so in fact, if this is to go back to an empty state
afterwards, then you had some random piece
of information here, which you're putting back
into a nonrandom state.
So somehow you have to get rid of randomness.
It's like getting rid of randomness,
because this could have been an L just as equally likely as it
ended up being an R here.
Getting rid of randomness is hard.
Getting rid of something you know
is in some state is not so hard.
And this is the important principle
we'll come back to as we look at reversible circuits
in a moment.
Yes, please.
Isn't the molecule moving slower than it was at the beginning?
Yes, it is.
So I'll draw my arrow a little shorter.
Thank you.
OK, so the module, the molecule moving slower
back at the initial state.
So this is done by erasing the memory.
And lo and behold, it is exactly that cost
that had been neglected in Maxwell's daemon's scenario.
He didn't actually think about the cost of the daemon
having to forget this last piece of information.
And why was he taking into account
the movement of the wall.
Right, because I'm really good about building
my wall on rollers here.
See?
Not just hinges.
And so compared with the cost of everything else,
it's a very small amount of energy.
It's also reversible.
Like imagine that the wall was heavy.
So I just let it drop.
And then I harvest that energy from the dropping,
and then I bring it back up.
So you can imagine.
I have a series of weights and cantilevers
to just move the energy of the position of the wall
between the two configurations.
So it can be made very, very small.
Of course, I've only considered a single molecule,
which is a very small amount of energy here.
Where maybe this is a very, very fast moving molecule.
Good question.
And then does it lose energy?
So the energy that it loses as it moves
is harnessed and is used to erase the memory.
Yes, so I could use the energy harvested from this piston
to erase this.
But eventually, I'm going to run out of steam.
So this is no longer a continuously operating engine,
like I might have had in another scenario.
So I can harvest energy as long as I can quantify--
net energy, as long as I can quantify the energy that it
takes to raise that bit.
That's what I would like to claim in just a moment
after I answer these two questions.
Yes, please.
Blue shirt.
Why do you lose so much energy?
Why would you necessarily run out of energy?
The molecule is not going to stop moving entirely.
It is true that the molecule's energy may eventually be lower
than, say, the friction of the piston in something like that.
So these now start to come into practical bounds.
But in computer science, we like to think about asymptotics
and what you could do in an idealized situation.
And the main point of this discussion
here is something that was picked up on,
and I'll describe that in just a moment.
Question, please.
Why is it important to start in with an empty memory?
Can you just say it's in some state,
then I just write it to a 0 or 1?
Yes.
And then that might not appear it again.
Yeah.
The question is, why is it important to start
with an empty state of the memory?
Why couldn't I just, for example,
erase it at the beginning?
You could do that.
It would be just a transient.
And another stage I could add.
And effectively, that would be making
step number eight equal to the first step instead.
So I claim the rest of the procedure
would just follow along.
So the connection between this and computation
was really made by this, Rolf Landauer in 1961.
Rolf Landauer, a very accomplished and famous
physicist from IBM in the heydays of IBM's impact
on the foundations of computer science,
formulated this beautiful observation.
That an n bit register--

notice now that we're talking about the scale
of some problem.
So we're now going to go to an asymptotic limit, where n is
some meaningfully large number.
And we know that this register has 2 to the n configurations.

So it's like n of these boxes of scratchpad over here.
And clearing this register to some standard state
is like, as we have just seen, compressing a gas in the sense
that all the configurations, no matter what
value you have in that n bit register,
must go to some definite final state,
whether it be all zeros or ones or whatever else you decide.
You lose the randomness that you had had.
And his observation is that this causes a flow of entropy.

And I realize I haven't described entropy for you.
And I hope you can inherit some of that understanding
from your own understanding of thermodynamics.
But the point is that this entropy that
was the randomness of this register
must go to the environment.

And this is due to--
I said we'd talk about thermodynamics--
this is due to the second law of thermo,
because entropy is conserved.
And that entropy must move somewhere.
And where it goes is into the environment.
And that means that the erasure must cost some energy, as many
of you observed.
And I want to quantify that again using
the laws of thermodynamics.
And as Rolf Landauer points out, it's kT log base e of 2,
because there are only two possibilities for each bit.
And this is per bit.

And this is of energy.

And this is due to the well-known result
that the heat pumped is equal to the temperature at which you
are pumping at times the change in entropy.
So the log 2 is just a measure of entropy,
the randomness of a bit.
And then the kT is the energy that
comes from the laws of thermodynamics.
And this k is Boltzmann's constant.

And it's a scale that's used to inherit the idea of Avogadro's
number in a mole.
Yes, please.
So especially regarding [? sort ?]
of S in the performed [INAUDIBLE]..
So what if we used a different definition of erasure?
As instead of reducing to one final state,
sort of just re-rolling the dice and randomizing that way.
Would that cost less energy?
OK, so the question is, what if we use
a different notion of erasure?
Maybe we re-roll the dice and use a different configuration.
But you see, there's entropy in this bit here.
It was either left or right.
That entropy had to go somewhere.
And there's no way around that.
So you could take a little less entropy, make it less random.
But then you get less energy harvested.
And so it all still comes back to you as this cost.
Other questions?
This is beautiful.
This result is known as the exorcism of Maxwell's daemon.

And when I learned about this, I was like, wow,
this is a really beautiful connection
between fundamental physics and computation.
And so you might say, well, how much
does it actually impact computation
that we use around ourselves today in our daily work?
And so the answer is modern computers
actually consume something like 500 kT log
2 joules per bit erased.

It might not seem like very much.
But you erase a lot of bits all the time
in a modern, like Intel, processor.
Now, if you were a DNA computer, that
can work in principle at 1.5 times 10
to the minus 19 joules per operation,
which is corresponds to two adenosine triphosphate
phosphorylations.
So ATP is exchanging.

And this is approximately 120 kT log 2.
So it's not that far away from what
biological systems, maybe a factor of four or something.
So it's very respectable.

Can we do better?
Why do we need to expend energy like this?
We've had a persistent level of questioning about the need
to do erasure.
So I want to challenge you.
Could we build a computer that doesn't need to erase?
If we could build a computer that did not
need to erase information, maybe it
would not need to dissipate energy to do the computation.
And this brings us to the third topic of today's lecture,
the fact that you could actually do that in principle.
